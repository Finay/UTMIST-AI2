# Automatic Bring Up of SB3 PPO policy

I made a simple way to bring up your PPO policy to Tenstorrent.

## Installation
1. Add a copy of [this file](https://github.com/Finay/UTMIST-AI2/blob/main/user/ppo_autobringup.py) to your `user` directory.
2. Import `TTPolicy` from user.ppo_autobringup with `from user.ppo_autobringup import TTPolicy`
3. Initialize `TTPolicy` with your loaded PPO model within `SubmittedAgent`'s `_initialize` function with `self.tt_policy = TTPolicy(self.model)` (This must come after `self.model = PPO.load(...)`)
4. Replace `predict` calls to the PPO model with calls to the TTPolicy.
```python3
# Previously:
def predict(self, obs):
        action, _ = self.model.predict(obs)
        return action

# With TTPolicy
def predict(self, obs):
        action, _ = self.tt_policy.predict(obs)
        return action
```

## Features and Explanation
SB3's PPO consists of 2 networks, the Actor and Critic. The Actor network generates actions from observations, Critic network generates evaluations from observations: 
```
Actor network flow:
Observation -Actor Feature Extractor-> Features -Extractor Policy Network-> Action Latents -Action Net-> Actions
Critic network flow:
Observation -Critic Feature Extractor-> Features -Extractor Value Network-> Value Latents -Value Net-> Value
```
When we initialize PPO with 'MlpPolicy' as done in the starter code (`PPO("MlpPolicy", self.env, verbose=0)` in `my_agent.py`), the Policy Network is an Mlp with Tanh activations 
with default dimensions \[64, 64\] unless specified in `net_arch`. The Action Net is always single linear layer to convert the latent to the action dimension.
The feature extractor can be shared between the Actor and Critic. The Critic network's purpose is to help training, therefore it is unnecessary for inference.

`TTPolicy` brings up the Features to Action forward pass (Policy Network and Action Net) in `ttnn`. As opposed to simply replacing the pytorch implementation of each network with 
something along the lines of:
```python3
model.policy.mlp_extractor.policy_net = (Some TTNN policy net implementation)
model.policy.action_net = (Some TTNN action net implementation)
```
Which would involve SB3's overhead and multiple conversions between tenstorrent tensors and torch tensors. (There may be some confusion between the feature extractor and mlp_extractor, 
refer to [this resources](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html) for more info)

Unfortunately, since the feature_extractor isn't always a Mlp and even if it was, bringing up would be implementation dependent, that is beyond the scope of this project. The given 
ttnn guide in the starter code goes over how to bring up a Mlp feature extractor and combining that with this will effectively bring up your entire forward pass but there will be
and unnecessary conversion to a torch tensor and back in between the two parts.

### PCC Validation
`TTPolicy` has a built-in pcc check which can be turned on with passing `run_pcc_validation=True` to `predict` so your agent's `predict` would look like the following:
```python3
# With TTPolicy
def predict(self, obs):
        action, _ = self.tt_policy.predict(obs, run_pcc_validation=True)
        return action
```
Running a demo match with your agent in this state individually checks the pearson correlation coefficient between the PPO model's policy/action net and the brought up policy/action net
as well as the output of the entire forward pass through the Action network. By default, SB3's PPO implementation samples an action from a distribution generated by the forward pass 
but for validation we compare the tt_policy's predictions against the mode of the SB3 policy's distribution. However, even with this, the tt_policy cannot maintain a PCC above 0.99 
on every observation and we can instead observe that the the success rate is sufficient from the output logs.

